# app.py
# -----------------------------------------------------------
# ChatPDF (Streamlit + LangChain + Chroma + LCEL)
# - PDF ì—…ë¡œë“œ ì‹œ: ê¸°ì¡´ DB í´ë”ê°€ ìˆìœ¼ë©´ ì•ˆì „ ì‚­ì œ í›„ ì¬ìƒì„±(ìœˆë„ìš° íŒŒì¼ë½ ëŒ€ì‘)
# - ì§ˆë¬¸ ì…ë ¥ â†’ ìŠ¤í”¼ë„ˆ ëŒ€ê¸° â†’ RAGë¡œ ë‹µë³€ ì¶œë ¥
# -----------------------------------------------------------
# ì‚¬ì „ ì„¤ì¹˜:
# pip install -U streamlit langchain langchain-openai langchain-chroma python-dotenv pypdf

import os
import gc
import time
import shutil
import tempfile
import streamlit as st
from dotenv import load_dotenv

load_dotenv()  # .envì—ì„œ OPENAI_API_KEY ë¡œë“œ

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from operator import itemgetter

# =========================
# í˜ì´ì§€ ê¸°ë³¸ UI
# =========================
st.set_page_config(page_title="ChatPDF", page_icon="ğŸ“„", layout="centered")
st.title("ğŸ“„ ChatPDF (RAG with LCEL)")
st.write("---")

if not os.getenv("OPENAI_API_KEY"):
    st.warning("âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .envë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# ì„¸ì…˜ ìƒíƒœ
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "persist_dir" not in st.session_state:
    st.session_state.persist_dir = "./db/chromadb"
if "retriever" not in st.session_state:
    st.session_state.retriever = None

# =========================
# ì•ˆì „ ì‚­ì œ ìœ í‹¸ (ìœˆë„ìš° ë½ ëŒ€ì‘)
# =========================
def safe_rmtree(path: str, retries: int = 8, delay: float = 0.2):
    """ìœˆë„ìš° íŒŒì¼ë½ì„ ê³ ë ¤í•´ rmtreeë¥¼ ì—¬ëŸ¬ ë²ˆ ì¬ì‹œë„."""
    last_err = None
    for _ in range(retries):
        try:
            shutil.rmtree(path)
            return
        except PermissionError as e:
            last_err = e
            time.sleep(delay)
    if last_err:
        raise last_err

# =========================
# ì—…ë¡œë“œ â†’ ë¬¸ì„œ ë³€í™˜
# =========================
def pdf_to_documents(file) -> list:
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œê²½ë¡œì— ì €ì¥ í›„ í˜ì´ì§€/ì²­í¬ ë‹¨ìœ„ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜."""
    temp_dir = tempfile.TemporaryDirectory()
    temp_path = os.path.join(temp_dir.name, file.name)
    with open(temp_path, "wb") as f:
        f.write(file.getvalue())

    loader = PyPDFLoader(temp_path)
    pages = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=50,
    )
    docs = splitter.split_documents(pages)
    return docs

# =========================
# Chroma ì¬ìƒì„±(ìë™ ì´ˆê¸°í™”)
# =========================
def recreate_chroma(docs, persist_dir="./db/chromadb", collection_name="esg"):
    import chromadb
    from langchain_openai import OpenAIEmbeddings
    from langchain_chroma import Chroma

    # 1) ê°™ì€ ê²½ë¡œë¡œ PersistentClient ì—°ê²°
    client = chromadb.PersistentClient(path=persist_dir)

    # 2) ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ (íŒŒì¼ë½ ì—†ì´ ì•ˆì „)
    try:
        client.delete_collection(collection_name)
    except Exception:
        # ì—†ìœ¼ë©´ ë¬´ì‹œ
        pass

    # 3) ìƒˆ ì„ë² ë”© + ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (ë™ì¼ persist_dir ì¬ì‚¬ìš©)
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        collection_name=collection_name,
        client=client,                 # â† ì¤‘ìš”: ê°™ì€ client ì‚¬ìš©
        persist_directory=persist_dir  # â† ê²½ë¡œ ìœ ì§€
    )
    return vectorstore

# =========================
# LCEL ì²´ì¸ (ì§ˆë¬¸â†’ê²€ìƒ‰â†’í”„ë¡¬í”„íŠ¸â†’LLMâ†’íŒŒì‹±)
# =========================
def build_lcel_chain(vectorstore, model_name="gpt-3.5-turbo"):
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    prompt = PromptTemplate.from_template(
        """
        ë„ˆëŠ” ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ìœ ìš©í•œ AIì•¼.
        ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì¤˜.

        ë¬¸ì„œ:
        {context}

        ì§ˆë¬¸:
        {question}
        """
    )

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    llm = ChatOpenAI(model=model_name, temperature=0)
    parser = StrOutputParser()

    # ì…ë ¥: {"question": "..."} ë¥¼ ê¸°ëŒ€
    # questionì€ ê·¸ëŒ€ë¡œ ë³´ì¡´, contextëŠ” questionâ†’retrieverâ†’formatìœ¼ë¡œ ìƒì„±
    chain = (
        {
            "question": RunnablePassthrough() | itemgetter("question"),
            "context": itemgetter("question") | retriever | RunnableLambda(format_docs),
        }
        | prompt
        | llm
        | parser
    )
    return chain

# =========================
# ì—…ë¡œë“œ UI & ìë™ ì¸ë±ì‹±
# =========================
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

if uploaded_file is not None:
    with st.spinner("ğŸ“š ì—…ë¡œë“œí•œ PDFë¥¼ ì½ê³  ì„ë² ë”©ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        docs = pdf_to_documents(uploaded_file)
        st.session_state.vectorstore = recreate_chroma(
            docs,
            persist_dir=st.session_state.persist_dir,
            collection_name="esg"
        )
        st.session_state.retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 3})
    #st.success(f"ì¸ë±ì‹± ì™„ë£Œ! (ì´ ì²­í¬ ìˆ˜: {len(docs)})")
    st.info("ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì•„ë˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•´ ë³´ì„¸ìš”.")
st.write("---")

# =========================
# ì§ˆë¬¸ ì…ë ¥ â†’ ë‹µë³€ ì¶œë ¥
# =========================
st.header("PDFì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if st.button("ì§ˆë¬¸í•˜ê¸°"):
    if uploaded_file is None:
        st.warning("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    elif st.session_state.vectorstore is None:
        st.warning("ì¸ë±ì‹±ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    elif not question.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ğŸ§  ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            chain = build_lcel_chain(st.session_state.vectorstore, model_name="gpt-3.5-turbo")
            answer = chain.invoke({"question": question})
        st.write(answer)

# # =========================
# # ë””ë²„ê·¸ ì •ë³´ (ì„ íƒ)
# # =========================
# with st.expander("ë””ë²„ê·¸ ì •ë³´"):
#     st.write("persist_dir:", st.session_state.persist_dir)
#     st.write("vectorstore ìƒì„±ë¨:", st.session_state.vectorstore is not None)
