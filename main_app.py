# app.py
# -----------------------------------------------------------
# ChatPDF (Streamlit + LangChain + Chroma-InMemory + LCEL)
# - PDF ì—…ë¡œë“œ â†’ ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥ (ë””ìŠ¤í¬ ì €ì¥ ì—†ìŒ)
# - ì§ˆë¬¸ ì…ë ¥ â†’ ë‹µë³€ ì¶œë ¥
# -----------------------------------------------------------

import os
import tempfile
import time
import shutil
import streamlit as st

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma  # âœ… Chroma ì‚¬ìš©
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from operator import itemgetter

# =========================
# Streamlit ê¸°ë³¸ UI êµ¬ì„±
# =========================
st.set_page_config(page_title="ChatPDF", page_icon="ğŸ“„", layout="centered")
st.title("ğŸ“„ ChatPDF (RAG with LCEL - In-Memory Chroma)")
st.write("---")

# API KEY í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    st.warning("âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .envë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "retriever" not in st.session_state:
    st.session_state.retriever = None

# =========================
# PDF â†’ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
# =========================
def pdf_to_documents(file) -> list:
    """ì—…ë¡œë“œëœ PDF íŒŒì¼ì„ ì„ì‹œê²½ë¡œì— ì €ì¥ í›„ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    temp_dir = tempfile.TemporaryDirectory()
    temp_path = os.path.join(temp_dir.name, file.name)
    with open(temp_path, "wb") as f:
        f.write(file.getvalue())

    loader = PyPDFLoader(temp_path)
    pages = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=50,
    )
    docs = splitter.split_documents(pages)
    return docs

# =========================
# In-Memory Chroma ìƒì„±
# =========================
def create_chroma_in_memory(docs):
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        collection_name="temp-inmemory"  # ë©”ëª¨ë¦¬ ëª¨ë“œì—ì„œëŠ” ì´ë¦„ë§Œ ì“°ë©´ ë¨
        # persist_directory ì§€ì •í•˜ì§€ ì•ŠìŒ â†’ ë©”ëª¨ë¦¬ ëª¨ë“œ
    )
    return vectorstore

# =========================
# LCEL ì²´ì¸ êµ¬ì„±
# =========================
def build_lcel_chain(vectorstore, model_name="gpt-3.5-turbo"):
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    prompt = PromptTemplate.from_template(
        """
        ë„ˆëŠ” ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ìœ ìš©í•œ AIì•¼.
        ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì¤˜.

        ë¬¸ì„œ:
        {context}

        ì§ˆë¬¸:
        {question}
        """
    )

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    llm = ChatOpenAI(model=model_name, temperature=0)
    parser = StrOutputParser()

    chain = (
        {
            "question": RunnablePassthrough() | itemgetter("question"),
            "context": itemgetter("question") | retriever | RunnableLambda(format_docs),
        }
        | prompt
        | llm
        | parser
    )
    return chain

# =========================
# PDF ì—…ë¡œë“œ â†’ ì„ë² ë”© ìƒì„±
# =========================
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

if uploaded_file is not None:
    with st.spinner("ğŸ“š PDFë¥¼ ì½ê³  ë©”ëª¨ë¦¬ì— ì„ë² ë”© ì¤‘ì…ë‹ˆë‹¤..."):
        docs = pdf_to_documents(uploaded_file)
        st.session_state.vectorstore = create_chroma_in_memory(docs)
        st.session_state.retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 3})
    st.info("âœ… ë¬¸ì„œê°€ ë©”ëª¨ë¦¬ì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì— ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
st.write("---")

# =========================
# ì§ˆë¬¸ ì…ë ¥ â†’ ë‹µë³€ ì¶œë ¥
# =========================
st.header("PDFì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if st.button("ì§ˆë¬¸í•˜ê¸°"):
    if uploaded_file is None:
        st.warning("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    elif st.session_state.vectorstore is None:
        st.warning("ì¸ë±ì‹±ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    elif not question.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ğŸ§  ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            chain = build_lcel_chain(st.session_state.vectorstore, model_name="gpt-3.5-turbo")
            answer = chain.invoke({"question": question})
        st.write(answer)
